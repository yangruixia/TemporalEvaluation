需要用到的python库：os,csv,spacy
且使用spacy库时需要额外加载一个英语语言模型“en_core_web_sm”（需要放在特定目录下）

一.获取所有语料数据（txt文件）
1.使用os.scandir()方法将语料目录下的所有txt文件名存为txt_files列表
2.使用os.path.join将txt_files列表中的每个文件名和语料目录名结合起来，获取实际可用路径

二.遍历，对每个txt文件进行处理(由于设备限制，代码中只处理了前100个文件)
1.使用spacy库中的nlp方法，遍历txt文件的每一行数据，查看是否有时间词（DATE/TIME）
2.若有时间词，则要进行依存距离的计算，反之，则忽略
3.使用nlp方法，再次查找中心词和时间词。
需要注意的是，时间词的长度可能大于1（如twenty years），就需要我们把每个时间词分隔开，避免混淆，如date_time_words=[[twenty,years,later],[today]]
4.由于语料数据的每个txt文件中，每一行即为一个单独的句子，因此此处可以先对每一行计算前置时间词数目，前置依存距离，后置时间词数目，后置依存距离。
基于存在时间词长度大于1的情况，此处依存距离的计算采取计算每个单词和中心词的依存距离，然后取平均值（average_dd）的算法。同时，若average_dd>=0，则证明是前置时间词，反之，则是后置时间词
5.每一行的前置时间词数目，前置依存距离，后置时间词数目，后置依存距离计算完毕后，只需将它们加起来，就可得到这个txt文件的前置时间词数目，前置依存距离，后置时间词数目，后置依存距离。前置率，后置率随即就可得出
6.丰富度的计算
date_time_words求得之后即可进行丰富度的计算。首先需要对每个单词进行大小写的统一，并使用join方法将[twenty,years,later]变为“twenty years later”
7.对时间词的频数进行排序

三。输出csv文件
使用csv库将数据写入到csv文件中





